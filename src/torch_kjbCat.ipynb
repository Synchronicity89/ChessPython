{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PyLinq import PyLinqData\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Move tensor to the GPU\n",
    "x = torch.tensor([1.0])\n",
    "x = x.to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for the en passant target 'e3' is 44 and the character is 'T'.\n"
     ]
    }
   ],
   "source": [
    "def en_passant_to_index(coordinate):\n",
    "    # Convert the file into a 0-based index (a=0, b=1, ..., h=7)\n",
    "    file_index = ord(coordinate[0].lower()) - ord('a')\n",
    "    # Convert the rank into a 0-based index (8=0, 7=1, ..., 1=7)\n",
    "    rank_index = 8 - int(coordinate[1])\n",
    "    # Calculate the index in the uncompressed FEN string\n",
    "    index = rank_index * 8 + file_index\n",
    "    # Determine the appropriate character based on the rank\n",
    "    en_passant_char = 'T' if coordinate[1] == '3' else 't'\n",
    "    return index, en_passant_char\n",
    "\n",
    "# Example usage:\n",
    "# For a white pawn moving from e2 to e4, the en passant target is e3.\n",
    "index, en_passant_char = en_passant_to_index('e3')\n",
    "print(f\"The index for the en passant target 'e3' is {index} and the character is '{en_passant_char}'.\")\n",
    "\n",
    "\n",
    "# # define the function to add the enpassant target rows\n",
    "# def addEnpassantTargets(origRow, fen, i):\n",
    "#     # create a string to represent the row\n",
    "#     # throw an error if i is not 2 or 6\n",
    "#     if(i != 2 and i != 5):\n",
    "#         raise ValueError(\"i must be 2 or 6\")\n",
    "#     row = origRow\n",
    "#     if(fen[3] != \"-\"):\n",
    "#         if(int(fen[3][1]) == i + 1 or int(fen[3][1]) == i):\n",
    "#             cols = \"abcdefgh\"\n",
    "#             index = cols.index(fen[3][0])\n",
    "#             # use the simplest notation to replace the character at the index with the t character\n",
    "#             row = row[:index] + (\"T\" if i == 2 else \"t\") + row[index + 1:]\n",
    "#     # set row to the 8 char substring of all starting at side * 8\n",
    "#     return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  KQkq  Converted:  KQkq  Reverted:  KQkq\n",
      "Original:  Kkq  Converted:  K-kq  Reverted:  Kkq\n",
      "Original:  KQk  Converted:  KQk-  Reverted:  KQk\n",
      "Original:  KQ  Converted:  KQ--  Reverted:  KQ\n",
      "Original:  K  Converted:  K---  Reverted:  K\n",
      "Original:  Q  Converted:  -Q--  Reverted:  Q\n",
      "Original:  k  Converted:  --k-  Reverted:  k\n",
      "Original:  q  Converted:  ---q  Reverted:  q\n",
      "Original:  -  Converted:  ----  Reverted:  -\n"
     ]
    }
   ],
   "source": [
    "def castlingRightsToString(castlingRights):\n",
    "    # Define the order of castling rights as they should appear in the string\n",
    "    orderedRights = \"KQkq\"\n",
    "    # Use list comprehension to check for each right in orderedRights and replace with \"-\" if absent\n",
    "    return ''.join(c if c in castlingRights else '-' for c in orderedRights)\n",
    "\n",
    "def revertCastlingRights(paddedRights):\n",
    "    # Filter out '-' characters and join the remaining characters to form the castling rights part of FEN\n",
    "    castlingRights = ''.join(c for c in paddedRights if c != '-')\n",
    "    # Return a dash if there are no castling rights, otherwise return the castling rights string\n",
    "    return castlingRights if castlingRights else '-'\n",
    "\n",
    "# test these functions with a variety of inputs such as KQkq, Kkq, KQk, KQ, K, Q, k, q, and the empty string\n",
    "# create a list of standard FEN castling rights strings which are variable length\n",
    "castlingRights = [\"KQkq\", \"Kkq\", \"KQk\", \"KQ\", \"K\", \"Q\", \"k\", \"q\", \"-\"]\n",
    "# use the map function to apply the function to each element of the list and then back again and compare the final result with the original\n",
    "# loop through each string in the list and outpt the result of the function and the original string to the console\n",
    "convertedRights = list(map(castlingRightsToString, castlingRights))\n",
    "revertedRights = list(map(revertCastlingRights, convertedRights))\n",
    "# go throough each string in the list and print the original string and the result of the function in detail\n",
    "for i in range(len(castlingRights)):\n",
    "    print(\"Original: \", castlingRights[i], \" Converted: \", convertedRights[i], \" Reverted: \", revertedRights[i])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate some new categorical data for a new neural network\n",
    "# the new data has enough columns to represent a variation of FEN notation for a chess board, where the / is left out because\n",
    "# each row is represented in full without any compression of the empty squares\n",
    "# here is an example of a normal FEN notation: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
    "# define a function to take the normal notation and convert it to a 64 column representation without slashes plus\n",
    "# The full move and half move counts are discarded, but the move turn and castling rights are preserved\n",
    "# here is an example of the conversion of the starting position: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1 to the new format\n",
    "# rnbqkbnrpppppppp00000000000000000000000000000000PPPPPPPPRNBQKBNRwKQkq\n",
    "# the first 64 columns represent the board, and the last 5 columns represent the move turn and castling rights\n",
    "# the new data has 69 columns\n",
    "def convertFEN(fen):\n",
    "    # split the FEN into its 6 components\n",
    "    fen = fen.split(\" \")\n",
    "    # split the board into its 8 rows\n",
    "    board = fen[0].split(\"/\")\n",
    "    # create a new board with the rows combined and the empty squares compressed\n",
    "    newBoard = \"\"\n",
    "    for i in range(0, 8):\n",
    "        # call a function to add the enpassant target rows if i has the value of 2 or 5\n",
    "        for j in range(0, len(board[i])):\n",
    "            if board[i][j].isdigit():\n",
    "                for k in range(0, int(board[i][j])):\n",
    "                    newBoard += \"0\"\n",
    "            else:\n",
    "                newBoard += board[i][j]\n",
    "        # if i == 2 or i == 5:\n",
    "        #     # pass the last 8 chars of newBoard into the enpassant function, then replace the last 8 chars of newBoard with the result\n",
    "        #     newBoard = newBoard[:-8] + addEnpassantTargets(newBoard[-8:], fen, i)\n",
    "            \n",
    "    if fen[3] != \"-\":\n",
    "        index, charEN = en_passant_to_index(fen[3])\n",
    "        newBoard = newBoard[:index] + charEN + newBoard[index + 1:]\n",
    "\n",
    "        # if fen[3][1] == \"3\":\n",
    "        #     newBoard = newBoard[:index - 8] + \"T\" + newBoard[index - 7:]\n",
    "        # else:\n",
    "        #     newBoard = newBoard[:index + 8] + \"t\" + newBoard[index + 9:]\n",
    "\n",
    "    # add the columns for move turn and castling rights\n",
    "    newBoard += fen[1] + castlingRightsToString(fen[2])\n",
    "    return newBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revertFEN(custom_fen):\n",
    "    # Find the en passant target and replace 't' or 'T' with '0'\n",
    "    en_passant_target = '-'\n",
    "    if 'T' in custom_fen or 't' in custom_fen:\n",
    "        en_passant_char = 'T' if 'T' in custom_fen else 't'\n",
    "        en_passant_target_index = custom_fen.index(en_passant_char)\n",
    "        # Convert the index to a coordinate\n",
    "        file = chr((en_passant_target_index % 8) + ord('a'))\n",
    "        rank = str(8 - (en_passant_target_index // 8))\n",
    "        en_passant_target = file + rank\n",
    "        # Replace 't' or 'T' with '0'\n",
    "        custom_fen = custom_fen[:en_passant_target_index] + '0' + custom_fen[en_passant_target_index+1:]\n",
    "\n",
    "    # Initialize an empty list to hold the standard FEN ranks\n",
    "    ranks = []\n",
    "    # Process each rank in the custom FEN\n",
    "    for rank_start in range(0, 64, 8):\n",
    "        # Extract the current rank\n",
    "        rank = custom_fen[rank_start:rank_start + 8]\n",
    "        # Replace zeros with the appropriate number of empty squares\n",
    "        standard_rank = ''\n",
    "        empty_count = 0\n",
    "        for char in rank:\n",
    "            if char == '0':\n",
    "                empty_count += 1\n",
    "            else:\n",
    "                if empty_count > 0:\n",
    "                    standard_rank += str(empty_count)\n",
    "                    empty_count = 0\n",
    "                standard_rank += char\n",
    "        if empty_count > 0:\n",
    "            standard_rank += str(empty_count)\n",
    "        # Add the processed rank to the list\n",
    "        ranks.append(standard_rank)\n",
    "    # Join the ranks with slashes to form the piece placement part of the standard FEN\n",
    "    piece_placement = '/'.join(ranks)\n",
    "    # Extract the remaining parts of the custom FEN\n",
    "    move_turn = custom_fen[64]\n",
    "    castling_rights = custom_fen[65:69].replace('-', '')\n",
    "    # If there are no castling rights, represent with a dash\n",
    "    castling_rights = castling_rights if castling_rights else '-'\n",
    "    # Assemble the standard FEN\n",
    "    return f\"{piece_placement} {move_turn} {castling_rights} {en_passant_target}\"\n",
    "\n",
    "# Test the revertFEN function with the given custom FEN\n",
    "standard_fen = \"r1bqkbnr/ppp1pppp/n7/3pP3/8/8/PPPP1PPP/RNBQKBNR w KQkq d6 0 3\"\n",
    "custom_fen = convertFEN(standard_fen)\n",
    "indexOfEnpassant,_ = en_passant_to_index('d6')\n",
    "characterAtIndex = custom_fen[indexOfEnpassant]\n",
    "rev_custom_fen = revertFEN(custom_fen)\n",
    "\n",
    "assert len(custom_fen) == 69 # 64 for the board, 1 for move turn, 4 for castling rights\n",
    "assert custom_fen[indexOfEnpassant] == 't'\n",
    "assert rev_custom_fen in standard_fen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r0bqkbnrppp0ppppn00t0000000pP0000000000000000000PPPP0PPPRNBQKBNRwKQkq\n",
      "r0bqkbnrppp0p0ppn00P00000000000000000pP0N00000T0PPPP0P0PR0BQKBNRbKQkq\n",
      "rnbqkbnrpppppppp00000000000000000000000000000000PPPPPPPPRNBQKBNRwKQkq\n",
      "00kr0bnrpppqp0ppn00P000000000b00000P0B00N00000p0PPPQ0P0PR000KBNRwKQ--\n"
     ]
    }
   ],
   "source": [
    "# test the function on the starting position\n",
    "# test with a different FEN where a move sequence E2E4, etc that leads to a board position where enpassant is possible\n",
    "fen1 = \"r1bqkbnr/ppp1pppp/n7/3pP3/8/8/PPPP1PPP/RNBQKBNR w KQkq d6 0 3\"\n",
    "newBoard1 = convertFEN(fen1)\n",
    "print(newBoard1)\n",
    "\n",
    "fen2 = \"r1bqkbnr/ppp1p1pp/n2P4/8/5pP1/N7/PPPP1P1P/R1BQKBNR b KQkq g3 0 5\"\n",
    "newBoard2 = convertFEN(fen2)\n",
    "print(newBoard2)\n",
    "\n",
    "fen3 = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "newBoard3 = convertFEN(fen3)\n",
    "print(newBoard3)\n",
    "\n",
    "fen4 = \"2kr1bnr/pppqp1pp/n2P4/5b2/3P1B2/N5p1/PPPQ1P1P/R3KBNR w KQ - 5 9\"\n",
    "newBoard4 = convertFEN(fen4)\n",
    "print(newBoard4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           CustomFEN\n",
      "0  rnbqkbnrpppppppp000000000000000000000000000000...\n",
      "1  rnbqkbnrpppppppp00000000000000000000P000000000...\n",
      "2  r0bqkbnrppppppppn0000000000000000000P000000000...\n",
      "3  r0bqkbnrppppppppn00000000000P00000000000000000...\n",
      "4  r0bqkbnrppp0ppppn00t0000000pP00000000000000000...\n",
      "                                           CustomFEN\n",
      "0  rnbqkbnrpppppppp00000000000000000000P000000000...\n",
      "1  r0bqkbnrppppppppn0000000000000000000P000000000...\n",
      "2  r0bqkbnrppppppppn00000000000P00000000000000000...\n",
      "3  r0bqkbnrppp0ppppn00t0000000pP00000000000000000...\n",
      "4  r0bqkbnrppp0ppppn00P00000000000000000000000000...\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-chess\n",
    "import chess\n",
    "import chess.pgn\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Assuming convertFEN is defined as before and working correctly\n",
    "\n",
    "# Sample PGN string for demonstration\n",
    "pgn_string = \"\"\"\n",
    "[Event \"NicosiaKyrenia's Study: Enpassant\"]\n",
    "[Site \"https://lichess.org/study/WCSL1Ul1/YZoxPnWI\"]\n",
    "[Result \"*\"]\n",
    "[Variant \"Standard\"]\n",
    "[ECO \"B00\"]\n",
    "[Opening \"Lemming Defense\"]\n",
    "[Annotator \"https://lichess.org/@/NicosiaKyrenia\"]\n",
    "[UTCDate \"2024.02.18\"]\n",
    "[UTCTime \"01:03:04\"]\n",
    "\n",
    "1. e4 Na6 2. e5 d5 3. exd6 f5 4. Na3 f4 5. g4 fxg3 *\n",
    "\"\"\"\n",
    "\n",
    "# Initialize pandas DataFrames for training data and labels\n",
    "training_data = pd.DataFrame(columns=['CustomFEN'])\n",
    "labels_data = pd.DataFrame(columns=['CustomFEN'])\n",
    "\n",
    "# Read the PGN\n",
    "pgn = io.StringIO(pgn_string)\n",
    "game = chess.pgn.read_game(pgn)\n",
    "\n",
    "# Initialize a board from the game\n",
    "board = game.board()\n",
    "\n",
    "for move in game.mainline_moves():\n",
    "    # Generate standard FEN before the move\n",
    "    fen_before = board.fen()\n",
    "    # Convert to custom FEN and store as training data\n",
    "    custom_fen_before = convertFEN(fen_before)\n",
    "    training_data = pd.concat([training_data, pd.DataFrame({'CustomFEN': [custom_fen_before]})], ignore_index=True)\n",
    "    \n",
    "    # Apply the move on the board\n",
    "    board.push(move)\n",
    "    \n",
    "    # Generate standard FEN after the move\n",
    "    fen_after = board.fen()\n",
    "    # Convert to custom FEN and store as label data\n",
    "    custom_fen_after = convertFEN(fen_after)\n",
    "    labels_data = pd.concat([labels_data, pd.DataFrame({'CustomFEN': [custom_fen_after]})], ignore_index=True)\n",
    "    # custom_fen_after = convertFEN(fen_after)\n",
    "    # labels_data = labels_data.append({'CustomFEN': custom_fen_after}, ignore_index=True)\n",
    "\n",
    "# For demonstration, print the first few rows of each DataFrame\n",
    "print(training_data.head())\n",
    "print(labels_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 square possibilities: ['R', 'N', 'B', 'Q', 'K', 'r', 'n', 'b', 'q', 'k', '0']\n",
      "Move turn possibilities: ['w', 'b']\n",
      "First castling right possibility (King-side for white): ['K', '-']\n"
     ]
    }
   ],
   "source": [
    "# This the following commented code is the general approach to go through the \n",
    "# training data and labels and convert the data to ordinal encoding\n",
    "# convert the data into ordinal encoding for all columns\n",
    "# for i in range(0, 69):\n",
    "#     uniqueValues = np.unique(training_data.iloc[:, i]) # this line finds the unique values in the column\n",
    "#     for j in range(0, len(uniqueValues)):\n",
    "#         training_data.iloc[:, i] = np.where(training_data.iloc[:, i] == uniqueValues[j], j, training_data.iloc[:, i])\n",
    "#         labels_data.iloc[:, i] = np.where(labels_data.iloc[:, i] == uniqueValues[j], j, labels_data.iloc[:, i])\n",
    "\n",
    "# however, it is possible to synthesize the possible unique values for each column keeping in mind these rules:\n",
    "# There are no pawns possible in rows 1 and 8\n",
    "# Bishops can only be only be in the same color square as the starting square (not sure if that will help with ordinal encoding)\n",
    "# There are only a few possible values for the castling rights and the move turn\n",
    "# The enpassant target can only be in rows 3 and 6\n",
    "# The enpassant target can only be in the same chess board column as the last move's pawn move (not sure if that will help with ordinal encoding)\n",
    "# define a function to synthesize the possible unique values for each column of the training data and labels\n",
    "# the function will return a list of the possible unique values for each column (don't know why it would be a list of lists)\n",
    "# the function uniqueValuesPossibleForColumn will take a record and the column index as input and return the possible unique values for that column\n",
    "# for i in range(0, 69):\n",
    "#     uniqueValues = uniqueValuesPossibleForColumn(training_data.iloc[0, :], i)\n",
    "\n",
    "def synthesize_custom_ordinal_values_final():\n",
    "    ordinal_values = {}\n",
    "\n",
    "    # Define all possible pieces, including uppercase for white and lowercase for black\n",
    "    pieces_general = ['R', 'N', 'B', 'Q', 'K', 'P', 'r', 'n', 'b', 'q', 'k', 'p', '0']\n",
    "    pieces_no_pawns = ['R', 'N', 'B', 'Q', 'K', 'r', 'n', 'b', 'q', 'k', '0']  # Exclude pawns in rows 1 and 8\n",
    "    pieces_en_passant = ['R', 'N', 'B', 'Q', 'K', 'P', 'r', 'n', 'b', 'q', 'k', 'p', '0', 'T', 't']  # Include 't' for en passant\n",
    "    \n",
    "    # Assign possible values for each board square\n",
    "    for i in range(64):\n",
    "        row = i // 8 + 1\n",
    "        if row in [1, 8]:\n",
    "            ordinal_values[i] = pieces_no_pawns\n",
    "        elif row in [3, 6]:  # Corrected rows for potential en passant targets, reflecting actual play possibilities\n",
    "            ordinal_values[i] = pieces_en_passant\n",
    "        else:\n",
    "            ordinal_values[i] = pieces_general\n",
    "\n",
    "    # Move turn possibilities\n",
    "    ordinal_values[64] = ['w', 'b']\n",
    "    \n",
    "    # Castling rights - specific to each right, with '-' indicating the absence of the right\n",
    "    ordinal_values[65] = ['K', '-']  # King-side for white\n",
    "    ordinal_values[66] = ['Q', '-']  # Queen-side for white\n",
    "    ordinal_values[67] = ['k', '-']  # King-side for black\n",
    "    ordinal_values[68] = ['q', '-']  # Queen-side for black\n",
    "    \n",
    "    return ordinal_values\n",
    "\n",
    "ordinal_values_final = synthesize_custom_ordinal_values_final()\n",
    "\n",
    "# Example of accessing possible values for specific squares or data points\n",
    "print(\"A1 square possibilities:\", ordinal_values_final[0])  # Example for A1 square\n",
    "print(\"Move turn possibilities:\", ordinal_values_final[64]) # Example for move turn\n",
    "print(\"First castling right possibility (King-side for white):\", ordinal_values_final[65]) # Example for first castling right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some of the custom FENs with the ordinal encoding to make sure the encoding is working correctly\n",
    "# for each custom FEN, go through each character and confirm that the character is in the possible unique values for that column\n",
    "# if it is not, print an error message\n",
    "\n",
    "for fen in [newBoard1, newBoard2, newBoard3, newBoard4]:\n",
    "    for i, char in enumerate(fen):\n",
    "        if char not in ordinal_values_final[i]:\n",
    "            print(f\"Error: {char} not in possible values for column {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# # Placeholder for the convertFEN function you provided earlier\n",
    "# def convertFEN(fen):\n",
    "#     # Your convertFEN function here\n",
    "#     custom_fen = convertFEN(fen)\n",
    "#     return custom_fen\n",
    "\n",
    "# Function to encode a custom FEN string to numerical format\n",
    "def encode_custom_fen(custom_fen):\n",
    "    # Implement the encoding logic here, using the ordinal_values mapping\n",
    "    # This is a placeholder function; you'll need to replace it with your actual encoding logic\n",
    "    # encoded = np.array([ordinal_values[char] for char in custom_fen])\n",
    "    # define a dictionary to hold the ordinal values for each piece.  Use negative numbers for black, and positive for white.\n",
    "    # pawns are 1, knights are 3, bishops 3.5, rooks 5, queens 9, and kings 15\n",
    "    pieceValues = {'R': 5, 'N': 3, 'B': 3.5, 'Q': 9, 'K': 15, 'P': 1, 'T': 1.3, 'r': -5, 'n': -3, 'b': -3.5, 'q': -9, 'k': -15, 'p': -1, 't': -1.3, '0': 0}\n",
    "    moveTurn = {'w': 1, 'b': -1}\n",
    "    # in general castlingRights = {'K': 1, 'Q': 1, 'k': -1, 'q': -1, '-': 0}\n",
    "    # however it needs to be split into 4 separate dictionaries\n",
    "    castlingRights1 = {'K': 1, '-': 0}\n",
    "    castlingRights2 = {'Q': 1, '-': 0}\n",
    "    castlingRights3 = {'k': -1, '-': 0}\n",
    "    castlingRights4 = {'q': -1, '-': 0}\n",
    "    # convert the custom FEN to a list of floats in a numpy array\n",
    "    encoded = (np.array([pieceValues[char] for char in custom_fen[:64]] + \n",
    "                        [moveTurn[custom_fen[64]]] + \n",
    "                        [castlingRights1[char] for char in custom_fen[65]] +\n",
    "                        [castlingRights2[char] for char in custom_fen[66]] +\n",
    "                        [castlingRights3[char] for char in custom_fen[67]] +\n",
    "                        [castlingRights4[char] for char in custom_fen[68]]))\n",
    "    # make sure its a float array\n",
    "    encoded = encoded.astype(float)    \n",
    "\n",
    "    return encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unencode_custom_fen(encoded_custom_fen):\n",
    "    # Implement the decoding logic here, using the ordinal_values mapping\n",
    "    # This is a placeholder function; you'll need to replace it with your actual decoding logic\n",
    "    # decoded = np.array([ordinal_values[char].index(char) for char in custom_fen])\n",
    "    # define a dictionary to hold the ordinal values for each piece.  Use negative numbers for black, and positive for white.\n",
    "    # pawns are 1, knights are 3, bishops 3.5, rooks 5, queens 9, and kings 15\n",
    "    pieceValues = {5: 'R', 3: 'N', 3.5: 'B', 9: 'Q', 15: 'K', 1: 'P', 1.3: 'T', -5: 'r', -3: 'n', -3.5: 'b', -9: 'q', -15: 'k', -1: 'p', -1.3: 't', 0: '0'}\n",
    "    moveTurn = {1: 'w', -1: 'b'}\n",
    "    # in general castlingRights = {1: 'K', 1: 'Q', -1: 'k', -1: 'q', 0: '-'}\n",
    "    # but must be split into 4 separate dictionaries\n",
    "    castlingRights1 = {1: 'K', 0: '-'}\n",
    "    castlingRights2 = {1: 'Q', 0: '-'}\n",
    "    castlingRights3 = {-1: 'k', 0: '-'}\n",
    "    castlingRights4 = {-1: 'q', 0: '-'}\n",
    "\n",
    "    # manually loop and convert the floats to the correct characters\n",
    "    decoded = \"\"\n",
    "    for i in range(0, 64):\n",
    "        decoded += pieceValues[encoded_custom_fen[i]]\n",
    "    decoded += moveTurn[encoded_custom_fen[64]]\n",
    "    decoded += castlingRights1[encoded_custom_fen[65]]\n",
    "    decoded += castlingRights2[encoded_custom_fen[66]]\n",
    "    decoded += castlingRights3[encoded_custom_fen[67]]\n",
    "    decoded += castlingRights4[encoded_custom_fen[68]]\n",
    "                              \n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of bad records is:  0\n",
      "The list of bad indices is:  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ChessDataset class definition\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# load csv called ../X_training_Y_Labels_Comment.csv and save the first column as training_data.csv and \n",
    "# the second column as labels_data.csv\n",
    "\n",
    "# Load and preprocess the data, may be able to comment out after initial run\n",
    "training_csv = '..\\X_training_Y_Labels_Comment.csv'\n",
    "# read it into numpy or pandas\n",
    "training_df = pd.read_csv(training_csv)\n",
    "\n",
    "# loop manually through each record to spot any anomalies\n",
    "badCount = 0\n",
    "indexListOfBad = []\n",
    "# for i in range(0, len(training_df)):\n",
    "#     # don't print anything unless there is an anomaly\n",
    "#     # convert each FEN, 2 per record, to a custom FEN and then encode it\n",
    "#     # each record consist of 2 FENs, the first is the input and the second is the output\n",
    "#     # extract the first FEN\n",
    "#     firstFEN = training_df.iloc[i, 0]\n",
    "#     # and the second FEN\n",
    "#     secondFEN = training_df.iloc[i, 1]\n",
    "#     # convert the first FEN to a custom FEN and then encode it\n",
    "#     convFirstFEN = convertFEN(firstFEN)\n",
    "#     encConvFirstFEN = encode_custom_fen(convFirstFEN)\n",
    "#     # convert the second FEN to a custom FEN and then encode it\n",
    "#     convSecondFEN = convertFEN(secondFEN)\n",
    "#     encConvSecondFEN = encode_custom_fen(convSecondFEN)\n",
    "#     # convert encConvFirstFEN and encConvSecondFEN back to custom FENs\n",
    "#     custEncConvFirstFEN = unencode_custom_fen(encConvFirstFEN)\n",
    "#     custEncConvSecondFEN = unencode_custom_fen(encConvSecondFEN)\n",
    "#     # convert them back to a FENs\n",
    "#     regFirstFEN = revertFEN(custEncConvFirstFEN)\n",
    "#     regSecondFEN = revertFEN(custEncConvSecondFEN)\n",
    "#     # compare the original FEN with the new FEN.  If the new FEN is not \n",
    "#     # contained in the original FEN, increment badCount and add the index to the list\n",
    "#     # define a boolean to see if the new FEN is contained in the original FEN\n",
    "#     firstFENContained = regFirstFEN in firstFEN\n",
    "#     secondFENContained = regSecondFEN in secondFEN\n",
    "#     # if either is false increment badCount and add the index to the list\n",
    "#     if not firstFENContained or not secondFENContained:\n",
    "#         # TODO remove test code\n",
    "#         regSecondFEN = revertFEN(custEncConvSecondFEN)\n",
    "\n",
    "#         badCount += 1\n",
    "#         indexListOfBad.append(i)\n",
    "#     # of bad indices.  once done looping remove the bad records from the dataframe\n",
    "#     # and print the bad count and the list of bad indices\n",
    "\n",
    "print(\"The number of bad records is: \", badCount)\n",
    "print(\"The list of bad indices is: \", indexListOfBad)\n",
    "# remove the bad records from the dataframe\n",
    "training_df = training_df.drop(indexListOfBad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52154, 69)\n",
      "(52154, 69)\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.         -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.         -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.         -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06666667  0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.          0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333  0.         -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      " -0.2         0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.         -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06666667  0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.          0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.         -0.06666667 -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06666667  0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.          0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333  0.         -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      " -0.2         0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333  0.         -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      " -0.2         0.          0.          0.          0.          0.\n",
      "  0.          0.23333333  0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.          0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667  0.         -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.06666667  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667  0.         -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.06666667  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.         -0.06666667 -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06666667  0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.06666667  0.          0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.2         0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.         -0.06666667 -0.2         0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06666667  0.06666667\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.          0.          0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667  0.         -0.06666667\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.06666667  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333 -0.2        -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667  0.          0.\n",
      " -0.06666667 -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      "  0.         -0.06666667  0.          0.          0.          0.\n",
      "  0.          0.         -0.06666667  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.23333333  0.          0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333  0.         -0.23333333 -0.6        -1.         -0.23333333\n",
      " -0.2        -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      " -0.2         0.          0.          0.          0.          0.\n",
      "  0.          0.23333333  0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.          0.          0.33333333 -1.          1.\n",
      "  1.         -1.         -1.        ]\n",
      "[-0.33333333  0.         -0.23333333 -0.6        -1.         -0.23333333\n",
      "  0.         -0.33333333 -0.06666667 -0.06666667 -0.06666667 -0.06666667\n",
      "  0.         -0.06666667 -0.06666667 -0.06666667  0.          0.\n",
      " -0.2         0.          0.         -0.2         0.          0.\n",
      "  0.          0.23333333  0.          0.         -0.06666667  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.06666667  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.2         0.          0.\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.          0.06666667\n",
      "  0.06666667  0.06666667  0.16666667  0.2         0.23333333  0.6\n",
      "  1.          0.          0.          0.33333333  1.          1.\n",
      "  1.         -1.         -1.        ]\n"
     ]
    }
   ],
   "source": [
    "# define a list of strings to hold X_convertFEN and another for y_convertFEN\n",
    "X_convertFEN = []\n",
    "y_convertFEN = []\n",
    "# loop through each row of the dataframe and convert the FENs to custom FENs.  The first column contains X training data and the second column contains y labels\n",
    "for i in range(0, len(training_df)):\n",
    "    # convert each FEN, 2 per record, to a custom FEN and then encode it\n",
    "    # each record consist of 2 FENs, the first is the input and the second is the output\n",
    "    # extract the first FEN\n",
    "    firstFEN = training_df.iloc[i, 0]\n",
    "    # and the second FEN\n",
    "    secondFEN = training_df.iloc[i, 1]\n",
    "    # convert the first FEN to a custom FEN and then encode it\n",
    "    convFirstFEN = convertFEN(firstFEN)\n",
    "    convSecondFEN = convertFEN(secondFEN)\n",
    "    X_convertFEN.append(convFirstFEN)\n",
    "    y_convertFEN.append(convSecondFEN) \n",
    "\n",
    "# check that the length of the strings in both lists is always 69\n",
    "for i in range(0, len(X_convertFEN)):\n",
    "    if len(X_convertFEN[i]) != 69:\n",
    "        print(\"The length of the string in X_convertFEN at index \", i, \" is \", len(X_convertFEN[i]))\n",
    "    if len(y_convertFEN[i]) != 69:\n",
    "        print(\"The length of the string in y_convertFEN at index \", i, \" is \", len(y_convertFEN[i]))\n",
    "\n",
    "# convert the lists to numpy arrays by running each string through the encode_custom_fen function\n",
    "X_raw = np.array([encode_custom_fen(fen) for fen in X_convertFEN])\n",
    "y_raw = np.array([encode_custom_fen(fen) for fen in y_convertFEN])\n",
    "\n",
    "# choose a random index into the trainging_df and define a simple list of strings variable fens to hold three FENs starting from that index\n",
    "index = 0\n",
    "fens = []\n",
    "for i in range(index, index + 3):\n",
    "    fens.append(training_df.iloc[i, 0])\n",
    "    fens.append(training_df.iloc[i, 0])\n",
    "    fens.append(training_df.iloc[i, 0])\n",
    "\n",
    "# check that the shape of the arrays is (number of records, 69)\n",
    "print(X_raw.shape)\n",
    "print(y_raw.shape)\n",
    "\n",
    "# Find the min and max values from the training data (X)\n",
    "X_min = X_raw.min(axis=0)\n",
    "X_max = X_raw.max(axis=0)\n",
    "\n",
    "# Normalize both X and y using the Min-Max scaling method based on X's min and max\n",
    "X = 2 * ((X_raw - X_min) / (X_max - X_min)) - 1\n",
    "y = 2 * ((y_raw - X_min) / (X_max - X_min)) - 1\n",
    "\n",
    "# print about 10 records to make sure the normalization worked\n",
    "for i in range(0, 10):\n",
    "    print(X[i])\n",
    "    print(y[i])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "val_size = len(X) - train_size\n",
    "X_train, X_val = np.split(X, [train_size])\n",
    "y_train, y_val = np.split(y, [train_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = ChessDataset(X_train, y_train)\n",
    "val_dataset = ChessDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch to use the neural network\n",
    "import torch\n",
    "# proceed towards training the model to use ordinal encoded data\n",
    "# define the model usint Tanh activation function\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Define the first layer with input size 69 and output size 202 (double the original middle layer size)\n",
    "        self.layer1 = nn.Linear(69, 202)\n",
    "        # Define the new middle layer with input size 202 and output size 202 (doubled size)\n",
    "        self.layer2 = nn.Linear(202, 202)\n",
    "        # Define the third layer symmetric to the first layer\n",
    "        self.layer3 = nn.Linear(202, 202)\n",
    "        # Define the fourth layer with output size 69, symmetric to the input layer\n",
    "        self.layer4 = nn.Linear(202, 69)\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "# create the neural network\n",
    "neuralNetwork = NeuralNetwork()\n",
    "neuralNetwork.to(device)\n",
    "\n",
    "# create the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(neuralNetwork.parameters(), lr=0.0018)\n",
    "\n",
    "# train the neural network\n",
    "lossHistory = []\n",
    "# for epoch in range(1500):\n",
    "#     for i, data in enumerate(train_loader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = neuralNetwork(inputs.float())\n",
    "#         loss = criterion(outputs.float(), labels.float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     lossHistory.append(loss.item())\n",
    "#     print(\"Epoch: \", epoch, \" Loss: \", loss.item())\n",
    "\n",
    "# # save the neural network\n",
    "# torch.save(neuralNetwork.state_dict(), \"model.pt\")\n",
    "\n",
    "# # save the loss history\n",
    "# lossHistory = pd.DataFrame(lossHistory, columns=[\"Loss\"])\n",
    "# lossHistory.to_csv(\"loss_history.csv\", index=False)\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(lossHistory)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:  [-0.2989  0.0554 -0.0089 -0.0186 -0.9917 -0.0612 -0.0011 -0.3278 -0.0034\n",
      " -0.0377  0.0143  0.0612 -0.133  -0.0488 -0.0771 -0.0195  0.0037  0.015\n",
      " -0.2183 -0.0443 -0.0997  0.0136 -0.0599  0.0065  0.0283 -0.047   0.0052\n",
      " -0.0507  0.091   0.0148  0.0317 -0.0288  0.0092 -0.0159 -0.4428  0.0303\n",
      "  0.0225  0.2251 -0.2374 -0.0071 -0.0195  0.015   0.0436  0.0229  0.003\n",
      "  0.1577  0.0274  0.041   0.0778  0.0736  0.0115 -0.0686  0.2244  0.2903\n",
      "  0.1251 -0.0261  0.143  -0.0419 -0.0533  0.0407  0.9858 -0.0023  0.0733\n",
      "  0.379  -1.      0.9768  0.9565 -0.9998 -0.9994]\n",
      "The FEN with the best move is:  r3k2r/pp2nppp/2n5/3pP3/2q2Bb1/5N1P/PP2NQP1/R3K2R b KQkq - 0 7\n"
     ]
    }
   ],
   "source": [
    "# redo the using statements for inference, and then load the model\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import chess\n",
    "\n",
    "# Assuming NeuralNetwork, encode_custom_fen, convertFEN, X_min, and X_max are defined elsewhere\n",
    "\n",
    "class ChessAI:\n",
    "    def __init__(self, model_path):\n",
    "        self.neuralNetwork = NeuralNetwork()\n",
    "        self.neuralNetwork.load_state_dict(torch.load(model_path))\n",
    "        self.neuralNetwork.to(torch.device(\"cpu\"))\n",
    "\n",
    "    def predict(self, fen):\n",
    "        input = self.prepare_input(fen)\n",
    "        output = self.neuralNetwork(input).cpu().detach().numpy()\n",
    "        return output\n",
    "\n",
    "    def prepare_input(self, fen):\n",
    "        input = encode_custom_fen(convertFEN(fen))\n",
    "        input = 2 * ((input - X_min) / (X_max - X_min)) - 1\n",
    "        return torch.tensor(input).float()\n",
    "\n",
    "    def find_best_move(self, fen, output):\n",
    "        board = chess.Board(fen)\n",
    "        possibleMoves = list(board.legal_moves)\n",
    "        newFENs = [self.make_move(board, move) for move in possibleMoves]\n",
    "        encodedFENs = {fen: self.prepare_input(fen) for fen in newFENs}\n",
    "        minLossIndex, _ = min(enumerate(encodedFENs.values()), key=lambda item: self.calculate_loss(item[1], output))\n",
    "        return list(encodedFENs.keys())[minLossIndex]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_move(board, move):\n",
    "        board.push(move)\n",
    "        fen = board.fen()\n",
    "        board.pop()\n",
    "        return fen\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(predicted, actual):\n",
    "        # Ensure both predicted and actual are NumPy arrays before subtraction and squaring\n",
    "        predicted = predicted.numpy() if isinstance(predicted, torch.Tensor) else predicted\n",
    "        actual = actual.numpy() if isinstance(actual, torch.Tensor) else actual\n",
    "        return np.sum((predicted - actual) ** 2)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "chess_ai = ChessAI(\"model.pt\")\n",
    "chosenFEN = \"r3k2r/pp2nppp/2n5/3pP3/2q2Bb1/5N2/PP2NQPP/R3K2R w KQkq - 4 7\"\n",
    "output = chess_ai.predict(chosenFEN)\n",
    "print(\"Predicted Output: \", np.round(output, 4))\n",
    "best_move_fen = chess_ai.find_best_move(chosenFEN, output)\n",
    "print(\"The FEN with the best move is: \", best_move_fen)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r n b q k b n r\n",
       "p p p p p p p p\n",
       ". . . . . . . .\n",
       ". . . . . . . .\n",
       ". . . P . . . .\n",
       ". . . . . . . .\n",
       "P P P . P P P P\n",
       "R N B Q K B N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 195)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>"
      ],
      "text/plain": [
       "Board('rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the functions needed to show a chess board, so that I can play against the neural network\n",
    "# define a function to show a chess board\n",
    "def showBoard(fen):\n",
    "    # create a new board\n",
    "    board = chess.Board(fen)\n",
    "    \n",
    "    # show the board\n",
    "    display(board)\n",
    "\n",
    "    return board\n",
    "\n",
    "# define a function to play a game against the neural network\n",
    "\n",
    "def get_best_move(fen, neural_network, X_min, X_max, encoded_fens):\n",
    "    # Convert the FEN to a custom FEN and then encode it\n",
    "    conv_fen = convertFEN(fen)\n",
    "    enc_conv_fen = encode_custom_fen(conv_fen)\n",
    "    \n",
    "    # Normalize the input using the same scaling as the training data\n",
    "    input = 2 * ((enc_conv_fen - X_min) / (X_max - X_min)) - 1\n",
    "    input = torch.tensor(input).float()\n",
    "    \n",
    "    # Use the neural network to predict the output\n",
    "    output = neural_network(input).cpu().detach().numpy()\n",
    "    \n",
    "    # Reverse the normalization of the neural network output\n",
    "    output = (output + 1) / 2 * (X_max - X_min) + X_min\n",
    "    \n",
    "    # Calculate the loss for each possible move and find the one with the minimum loss\n",
    "    min_loss = float('inf')\n",
    "    best_move = None\n",
    "    \n",
    "    for fen, encoded_fen in encoded_fens.items():\n",
    "        # Reverse the normalization of the encoded converted FEN\n",
    "        encoded_fen = (encoded_fen + 1) / 2 * (X_max - X_min) + X_min\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = np.sum((output - encoded_fen) ** 2)\n",
    "        \n",
    "        # Update the best move if the loss is smaller\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            best_move = fen\n",
    "    \n",
    "    # Return the best move or matching FEN\n",
    "    return best_move\n",
    "\n",
    "# Define the necessary variables\n",
    "neural_network = NeuralNetwork()\n",
    "neural_network.load_state_dict(torch.load(\"model.pt\"))\n",
    "neural_network.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Define the encoded FENs dictionary\n",
    "encoded_fens = {}\n",
    "# display the board\n",
    "bd = showBoard(fens[0])\n",
    "# hook the event that is triggered when a move is made on the bd\n",
    "def on_move_made(move):\n",
    "    # get the new FEN\n",
    "    newFEN = bd.fen()\n",
    "    # convert the FEN to a custom FEN and then encode it\n",
    "    convFEN = convertFEN(newFEN)\n",
    "    encConvFEN = encode_custom_fen(convFEN)\n",
    "    # add the encoded converted FEN to the dictionary\n",
    "    encoded_fens[newFEN] = encConvFEN\n",
    "    # get the best move from the neural network\n",
    "    bestMove = get_best_move(newFEN, neural_network, X_min, X_max, encoded_fens)\n",
    "    # Print the best move\n",
    "    print(\"The best move to play against the neural network is:\", bestMove)\n",
    "# get the possible moves\n",
    "# hook the event to the on_move_made function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best move to play against the neural network is: None\n"
     ]
    }
   ],
   "source": [
    "# define fen_to_play from the board position just shown\n",
    "fen_to_play = bd.fen()\n",
    "\n",
    "# Get the best move from the neural network\n",
    "best_move = get_best_move(fen_to_play, neural_network, X_min, X_max, encoded_fens)\n",
    "\n",
    "# Print the best move\n",
    "print(\"The best move to play against the neural network is:\", best_move)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchPlay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
